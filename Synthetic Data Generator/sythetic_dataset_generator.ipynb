{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b05666e8-5992-453c-9059-1ee95710976f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b05666e8-5992-453c-9059-1ee95710976f",
        "outputId": "cf130f68-0025-4624-97c2-36b2a7657697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5738c4dd",
      "metadata": {
        "id": "5738c4dd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import re, pandas as pd\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to HuggingFace Hub\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "sXyiJJz7rnX7"
      },
      "id": "sXyiJJz7rnX7",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "59812e4b",
      "metadata": {
        "id": "59812e4b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# initialize OpenAI client\n",
        "openai = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a7893de2",
      "metadata": {
        "id": "a7893de2"
      },
      "outputs": [],
      "source": [
        "SYSTEM_MESSAGE = \"\"\"\n",
        "You are a Dataset generator assistant across various fields.\n",
        "Maintain a 70‚Äì30 positive/negative ratio.\n",
        "No null values.\n",
        "Return ONLY a valid JSON array (no markdown, no explanations, no extra text) of the records\n",
        "Do not include code fences or text outside JSON.\n",
        "\n",
        "example record format:\n",
        "[\n",
        "  {{ \"id\": 1, \"....\": \"....\", \"....\": \"...\", \"...\": \"...... }}\n",
        "]\n",
        "\n",
        "the above is just example for the json format, so by referencing this generate the synthetic data as per user domain specific request\n",
        "and last important thing Use exactly the same JSON schema (column names and order) across all chunks.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f4e5c28a",
      "metadata": {
        "id": "f4e5c28a"
      },
      "outputs": [],
      "source": [
        "MODEL_INFO = {\n",
        "    \"meta-llama/Llama-3.1-8B-Instruct\": {\"context\": 8192, \"tokens_per_row\": 40},\n",
        "    \"meta-llama/Llama-3.2-1B-Instruct\": {\"context\": 4096, \"tokens_per_row\": 35},\n",
        "    \"microsoft/Phi-4-mini-instruct\":    {\"context\": 8192, \"tokens_per_row\": 40},\n",
        "    \"google/gemma-3-270m-it\":           {\"context\": 2048, \"tokens_per_row\": 30},\n",
        "    \"Qwen/Qwen3-4B-Instruct-2507\":      {\"context\": 16384, \"tokens_per_row\": 40},\n",
        "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\": {\"context\": 4096, \"tokens_per_row\": 35},\n",
        "    \"gpt-4o-mini\":                       {\"context\": 16384, \"tokens_per_row\": 40},\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ea37b035",
      "metadata": {
        "id": "ea37b035"
      },
      "outputs": [],
      "source": [
        "def get_model_limits(model_id: str, total_rows: int):\n",
        "    \"\"\"Compute chunk size and safe token budget for the chosen model.\"\"\"\n",
        "    info = MODEL_INFO.get(model_id, {\"context\": 4096, \"tokens_per_row\": 40})\n",
        "    max_context = info[\"context\"]\n",
        "    tpr = info[\"tokens_per_row\"]\n",
        "    safe_output_tokens = int(max_context * 0.7)          # keep 30% headroom for prompt\n",
        "    chunk_size = max(20, min(total_rows, safe_output_tokens // tpr))\n",
        "    return chunk_size, safe_output_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9ee356e1",
      "metadata": {
        "id": "9ee356e1"
      },
      "outputs": [],
      "source": [
        "def extract_json_array(text):\n",
        "    match = re.search(r\"\\[.*\\]\", text, re.DOTALL)\n",
        "    if not match:\n",
        "        raise ValueError(\"No JSON array found in output.\")\n",
        "    return json.loads(match.group(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1faab052",
      "metadata": {
        "id": "1faab052"
      },
      "outputs": [],
      "source": [
        "def generate_chunk_openai(chunk_index, num_rows, user_request, model, max_tokens, schema=None, schema_example=None):\n",
        "    \"\"\"Generate one JSON chunk safely with dynamic token limit.\"\"\"\n",
        "    if schema_example:\n",
        "        schema_text = (\n",
        "            f\"Follow exactly this JSON structure and types:\\n\"\n",
        "            f\"{json.dumps(schema_example[:2], indent=2)}\\n\"\n",
        "            f\"Do not change key names or value types.\"\n",
        "        )\n",
        "    elif schema:\n",
        "        schema_text = f\"Use the exact same columns as these: {schema}.\"\n",
        "    else:\n",
        "        schema_text = \"Generate your own schema suitable for the dataset and keep it consistent for future chunks.\"\n",
        "\n",
        "    chunk_prompt = (\n",
        "        f\"{user_request}\\n\"\n",
        "        f\"Generate exactly {num_rows} unique JSON records.\\n\"\n",
        "        f\"IDs should start from {chunk_index + 1}.\\n\"\n",
        "        f\"{schema_text}\\n\"\n",
        "        f\"Return only valid JSON array.\"\n",
        "    )\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=0.2,\n",
        "        max_tokens=max_tokens,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
        "            {\"role\": \"user\", \"content\": chunk_prompt},\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    return extract_json_array(response.choices[0].message.content)\n",
        "    # except ValueError:\n",
        "    #     # Retry with half rows if model truncated output\n",
        "    #     if num_rows > 30:\n",
        "    #         print(f\"‚ö†Ô∏è  Chunk {chunk_index}: retrying with smaller size ({num_rows//2})...\")\n",
        "    #         return generate_chunk_openai(chunk_index, num_rows // 2, user_request, model, max_tokens, schema, schema_example)\n",
        "    #     raise"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# üîπ Global model cache (shared across threads)\n",
        "# ===========================================================\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "import torch\n",
        "\n",
        "MODEL_CACHE = {}\n",
        "\n",
        "def load_model_once(model_id):\n",
        "    \"\"\"\n",
        "    Load and quantize a model once, then reuse it for all threads.\n",
        "    Safe for inference because model weights are read-only.\n",
        "    \"\"\"\n",
        "    print(f\"üöÄ Loading model into cache: {model_id}\")\n",
        "\n",
        "    quant_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quant_config\n",
        "    )\n",
        "\n",
        "    generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        temperature=0.2,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "    # Store everything needed in cache\n",
        "    MODEL_CACHE[model_id] = {\n",
        "        \"model\": model,\n",
        "        \"tokenizer\": tokenizer,\n",
        "        \"generator\": generator\n",
        "    }\n",
        "    return MODEL_CACHE[model_id]\n",
        "\n",
        "\n",
        "def get_model(model_id):\n",
        "    \"\"\"\n",
        "    Retrieve the model/tokenizer/generator from cache,\n",
        "    loading it if not already present.\n",
        "    \"\"\"\n",
        "    if model_id not in MODEL_CACHE:\n",
        "        MODEL_CACHE[model_id] = load_model_once(model_id)\n",
        "    return MODEL_CACHE[model_id]\n"
      ],
      "metadata": {
        "id": "1N_aFyAXh7VZ"
      },
      "id": "1N_aFyAXh7VZ",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9356eadf",
      "metadata": {
        "id": "9356eadf"
      },
      "outputs": [],
      "source": [
        "import json, re\n",
        "\n",
        "def generate_chunk_hf(chunk_index, num_rows, user_request, model_id, max_tokens, schema=None, schema_example=None):\n",
        "    print(f\"üß© Using cached {model_id} for chunk {chunk_index}...\")\n",
        "\n",
        "    # ‚úÖ Retrieve model + tokenizer + generator from cache\n",
        "    cache = get_model(model_id)\n",
        "    generator = cache[\"generator\"]\n",
        "\n",
        "    schema_text = (\n",
        "        f\"Follow exactly this JSON structure and types:\\n{json.dumps(schema_example[:2], indent=2)}\"\n",
        "        if schema_example else\n",
        "        \"Generate your own schema suitable for the dataset and keep it consistent for future chunks.\"\n",
        "    )\n",
        "\n",
        "    prompt = (\n",
        "        f\"{user_request}\\n\"\n",
        "        f\"Generate exactly {num_rows} unique JSON records.\\n\"\n",
        "        f\"IDs should start from {chunk_index + 1}.\\n\"\n",
        "        f\"{schema_text}\\n\"\n",
        "        f\"Return only valid JSON array, no explanations.\"\n",
        "        f\"Output ONLY the JSON array ‚Äî no text before or after it.\"\n",
        "    )\n",
        "\n",
        "    out = generator(prompt, max_new_tokens=max_tokens)[0][\"generated_text\"]\n",
        "\n",
        "    match = re.search(r\"\\[.*\\]\", out, re.DOTALL)\n",
        "    if not match:\n",
        "        print(\"‚ö†Ô∏è Raw output (first 500 chars):\", out[:500])\n",
        "        raise ValueError(\"No JSON array found in model output.\")\n",
        "\n",
        "    json_text = match.group(0)\n",
        "\n",
        "    # ‚úÖ Trim anything after the final closing bracket\n",
        "    last_bracket = json_text.rfind(\"]\")\n",
        "    if last_bracket != -1:\n",
        "        json_text = json_text[:last_bracket + 1]\n",
        "\n",
        "    # ‚úÖ Parse JSON safely\n",
        "    try:\n",
        "        return json.loads(json_text)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚ö†Ô∏è JSON decode error: {e}\")\n",
        "        # try minimal repair (remove trailing commas, line breaks)\n",
        "        repaired = re.sub(r\",\\s*]\", \"]\", json_text)\n",
        "        repaired = repaired.replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n",
        "        try:\n",
        "            return json.loads(repaired)\n",
        "        except Exception as e2:\n",
        "            print(\"‚ö†Ô∏è Still invalid JSON. Returning empty list.\")\n",
        "            print(\"First 500 chars of broken output:\\n\", json_text[:500])\n",
        "            return []\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3da72833",
      "metadata": {
        "id": "3da72833"
      },
      "outputs": [],
      "source": [
        "def get_thread_count(total_rows):\n",
        "    if total_rows <= 500:\n",
        "        return 5\n",
        "    elif total_rows <= 2000:\n",
        "        return 6\n",
        "    elif total_rows <= 5000:\n",
        "        return 8\n",
        "    else:\n",
        "        return 10  # cap for huge datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8c873ea5",
      "metadata": {
        "id": "8c873ea5"
      },
      "outputs": [],
      "source": [
        "def generate_dataset_threaded(user_request,total_rows, model):\n",
        "\n",
        "    \"\"\"Full threaded dataset generator with model-adaptive chunk/token logic.\"\"\"\n",
        "    chunk_func=generate_chunk_openai if model.startswith('gpt') else generate_chunk_hf\n",
        "\n",
        "    # Determine per-model limits\n",
        "    chunk_size, safe_tokens = get_model_limits(model, total_rows)\n",
        "    max_workers = get_thread_count(total_rows)\n",
        "\n",
        "    print(f\"üöÄ Model: {model}\")\n",
        "    print(f\"üìä Total rows: {total_rows}\")\n",
        "    print(f\"üßÆ Chunk size: {chunk_size}, Token limit per chunk: {safe_tokens}\")\n",
        "    print(f\"üßµ Threads: {max_workers}\\n\")\n",
        "\n",
        "    # Compute per-thread splits\n",
        "    num_workers = max_workers\n",
        "    base_rows = total_rows // num_workers\n",
        "    remainder = total_rows % num_workers\n",
        "    rows_per_chunk = [base_rows + (1 if i < remainder else 0) for i in range(num_workers)]\n",
        "\n",
        "    all_records = []\n",
        "\n",
        "    # --- First chunk defines schema ---\n",
        "    first_chunk_rows = rows_per_chunk[0]\n",
        "    first_chunk = chunk_func(0, first_chunk_rows, user_request, model, safe_tokens)\n",
        "\n",
        "    # ‚úÖ Fallback if first chunk failed\n",
        "    if not first_chunk or len(first_chunk) == 0:\n",
        "        print(\"‚ö†Ô∏è First chunk failed ‚Äî retrying once with simplified prompt...\")\n",
        "        first_chunk = chunk_func(0, first_chunk_rows, f\"{user_request} (output only valid JSON)\", model, safe_tokens)\n",
        "\n",
        "    # ‚úÖ If still empty, fail gracefully\n",
        "    if not first_chunk or len(first_chunk) == 0:\n",
        "        raise ValueError(\"‚ùå Unable to generate a valid schema from the model output. Try smaller rows or different model.\")\n",
        "\n",
        "    all_records.extend(first_chunk)\n",
        "    schema = list(first_chunk[0].keys())\n",
        "    schema_example = first_chunk[:2]\n",
        "    print(f\"‚úÖ Schema captured: {schema}\\n\")\n",
        "\n",
        "    # --- Parallel generation for remaining chunks ---\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        futures = []\n",
        "        start_id = first_chunk_rows\n",
        "        for i in range(1, num_workers):\n",
        "            futures.append(\n",
        "                executor.submit(\n",
        "                    chunk_func, start_id, rows_per_chunk[i],\n",
        "                    user_request, model, safe_tokens, schema, schema_example\n",
        "                )\n",
        "            )\n",
        "            start_id += rows_per_chunk[i]\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            try:\n",
        "                result = future.result()\n",
        "                all_records.extend(result)\n",
        "                print(f\"‚úÖ Completed chunk ({len(result)} records)\")\n",
        "            except Exception as e:\n",
        "              import traceback\n",
        "              print(f\"‚ùå Error in chunk {i}: {e}\\n{traceback.format_exc()}\")\n",
        "\n",
        "\n",
        "    # --- Postprocess ---\n",
        "    df = pd.DataFrame(all_records)\n",
        "    df = df.reindex(columns=schema).fillna(\"N/A\").sort_values(by=\"id\", ignore_index=True)\n",
        "\n",
        "    # --- Top-up if under-produced ---\n",
        "    missing = total_rows - len(df)\n",
        "    while missing > 0:\n",
        "        print(f\"‚ö†Ô∏è Missing {missing} rows, generating top-up...\")\n",
        "        extra = chunk_func(total_rows, missing, user_request, model, safe_tokens, schema, schema_example)\n",
        "        df = pd.concat([df, pd.DataFrame(extra)], ignore_index=True)\n",
        "        df = df.reindex(columns=schema).fillna(\"N/A\").sort_values(by=\"id\", ignore_index=True)\n",
        "        missing = total_rows - len(df)\n",
        "\n",
        "    print(f\"\\n‚úÖ Final dataset: {len(df)} rows √ó {len(schema)} columns.\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, bitsandbytes as bnb\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-lVjl43nSOF",
        "outputId": "750f7cf5-f101-46f4-8464-5686ffc8739d"
      },
      "id": "w-lVjl43nSOF",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3e120d1d",
      "metadata": {
        "id": "3e120d1d"
      },
      "outputs": [],
      "source": [
        "def get_total_rows(user_request):\n",
        "    q = user_request.lower()\n",
        "    if \"basic\" in q:\n",
        "        return 500\n",
        "    elif \"medium\" in q:\n",
        "        return 2000\n",
        "    elif \"large\" in q:\n",
        "        return 5000\n",
        "    else:\n",
        "        return 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "6730573e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6730573e",
        "outputId": "d27c9e06-c21a-4545-91be-39d6783e2990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Model: gpt-4o-mini\n",
            "üìä Total rows: 5\n",
            "üßÆ Chunk size: 20, Token limit per chunk: 11468\n",
            "üßµ Threads: 5\n",
            "\n",
            "‚úÖ Schema captured: ['id', 'name', 'discovery_year', 'mass', 'radius', 'orbital_period', 'distance_from_earth', 'host_star', 'star_type', 'habitable_zone', 'discovery_method']\n",
            "\n",
            "‚úÖ Completed chunk (1 records)\n",
            "‚úÖ Completed chunk (1 records)\n",
            "‚úÖ Completed chunk (1 records)\n",
            "‚úÖ Completed chunk (1 records)\n",
            "\n",
            "‚úÖ Final dataset: 5 rows √ó 11 columns.\n",
            "   id                name  discovery_year  mass  radius  orbital_period  \\\n",
            "0   1         Kepler-186f            2014  0.55    1.10         130.600   \n",
            "1   2  Proxima Centauri b            2016  1.27    1.07          11.200   \n",
            "2   3  Proxima Centauri b            2016  1.17    1.07          11.200   \n",
            "3   4  Proxima Centauri b            2016  1.17    1.07          11.200   \n",
            "4   5         HD 209458 b            1999  0.69    1.35           3.524   \n",
            "\n",
            "   distance_from_earth         host_star star_type  habitable_zone  \\\n",
            "0               500.00        Kepler-186   M-dwarf            True   \n",
            "1                 4.24  Proxima Centauri   M-dwarf            True   \n",
            "2                 4.24  Proxima Centauri   M-dwarf            True   \n",
            "3                 4.24  Proxima Centauri   M-dwarf            True   \n",
            "4               159.00         HD 209458    G-type           False   \n",
            "\n",
            "  discovery_method  \n",
            "0          transit  \n",
            "1  radial velocity  \n",
            "2  radial velocity  \n",
            "3  radial velocity  \n",
            "4          transit  \n"
          ]
        }
      ],
      "source": [
        "user_request = \"Generate a dataset of exoplanets discovered between 2000 and till now with full details.\"\n",
        "model_id = \"gpt-4o-mini\"\n",
        "total_rows = get_total_rows(user_request)\n",
        "\n",
        "\n",
        "df = generate_dataset_threaded(\n",
        "    user_request=user_request,\n",
        "    total_rows=total_rows,\n",
        "    model=model_id\n",
        ")\n",
        "\n",
        "print(df.head())\n",
        "df.to_csv(\"synthetic_dataset.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import time\n",
        "import contextlib, io, traceback\n",
        "\n",
        "def generate_dataset(user_request, model_name, dataset_size, progress=gr.Progress()):\n",
        "    # ‚úÖ Step 1: validate user input immediately\n",
        "    if not user_request or user_request.strip() == \"\":\n",
        "        raise gr.Error(\"‚ö†Ô∏è Please enter a dataset request before generating.\")\n",
        "\n",
        "    try:\n",
        "        progress(0, desc=\"Initializing model and configuration...\")\n",
        "        time.sleep(0.8)\n",
        "\n",
        "        size_map = {\"Small\": 500, \"Medium\": 2000, \"Large\": 5000}\n",
        "        total_rows = size_map.get(dataset_size, 500)\n",
        "\n",
        "        progress(0.3, desc=f\"Generating {total_rows} rows using {model_name}...\")\n",
        "        time.sleep(1)\n",
        "\n",
        "        # Silence prints from threaded generator\n",
        "        with contextlib.redirect_stdout(io.StringIO()):\n",
        "            df = generate_dataset_threaded(\n",
        "                user_request=user_request,\n",
        "                total_rows=total_rows,\n",
        "                model=model_name\n",
        "            )\n",
        "\n",
        "        if not isinstance(df, pd.DataFrame):\n",
        "            raise TypeError(f\"Expected a DataFrame but got {type(df)}\")\n",
        "\n",
        "        # Save CSV\n",
        "        output_file = \"synthetic_dataset.csv\"\n",
        "        df.to_csv(output_file, index=False)\n",
        "\n",
        "        progress(0.9, desc=\"Finalizing dataset...\")\n",
        "        time.sleep(0.5)\n",
        "        progress(1, desc=\"‚úÖ Done!\")\n",
        "\n",
        "        details = (\n",
        "            f\"**Model:** {model_name}\\n\"\n",
        "            f\"**Dataset Size:** {dataset_size} ({total_rows} rows)\\n\"\n",
        "            f\"**User Request:** {user_request}\"\n",
        "        )\n",
        "\n",
        "        # ‚úÖ Return clean results\n",
        "        return details, df, output_file\n",
        "\n",
        "    except Exception as e:\n",
        "        # Print traceback but DO NOT update output boxes with error\n",
        "        print(\"‚ùå ERROR DURING GENERATION:\\n\", traceback.format_exc())\n",
        "        # Just raise popup error (stops execution, no red boxes)\n",
        "        raise gr.Error(f\"‚ùå Something went wrong during generation:\\n{e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Example Prompts ---\n",
        "examples = [\n",
        "    [\"Generate a dataset of exoplanets discovered between 2000 and now\"],\n",
        "    [\"Create a dataset of global earthquakes with magnitude >6.0 since 2010\"],\n",
        "    [\"Generate a dataset of top 100 AI companies with country, valuation, and sector\"]\n",
        "]\n",
        "\n",
        "# --- Gradio App Layout ---\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"## üß¨ Synthetic Dataset Generator\")\n",
        "    gr.Markdown(\"Easily generate structured datasets using LLM models.\")\n",
        "\n",
        "    # 1Ô∏è‚É£ User Request\n",
        "    user_input = gr.Textbox(\n",
        "        label=\"Dataset Request\",\n",
        "        placeholder=\"e.g., Generate a dataset of exoplanets discovered between 2000 and now\"\n",
        "    )\n",
        "\n",
        "    # 2Ô∏è‚É£ Model and Dataset Size\n",
        "    with gr.Row():\n",
        "        model_dropdown = gr.Dropdown(\n",
        "            choices=[\n",
        "                \"gpt-4o-mini\",\n",
        "                \"google/gemma-3-270m-it\",\n",
        "                \"Qwen/Qwen3-4B-Instruct-2507\",\n",
        "                \"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "                \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "                \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "                \"Qwen/Qwen2-1.5B-Instruct\",\n",
        "                \"microsoft/Phi-4-mini\"\n",
        "            ],\n",
        "            label=\"Select Model\",\n",
        "            value=\"gpt-4o-mini\"\n",
        "        )\n",
        "\n",
        "        size_dropdown = gr.Radio(\n",
        "          choices=[\"Small\", \"Medium\", \"Large\"],\n",
        "          label=\"Select Dataset Size\",\n",
        "          value=\"Small\",\n",
        "          info=\"* Small = 500 rows * Medium = 2,000 rows * Large = 5,000 rows\"\n",
        "      )\n",
        "\n",
        "    gr.Examples(\n",
        "        examples=examples,\n",
        "        inputs=[user_input],\n",
        "        label=\"Try one of these prompts!\"\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "    # 3Ô∏è‚É£ Generate Button\n",
        "    generate_btn = gr.Button(\"Generate Data\")\n",
        "\n",
        "    # 4Ô∏è‚É£ Progress + Details\n",
        "    generation_details = gr.Markdown(\"*(Click Generate to start...)*\")\n",
        "\n",
        "    # 5Ô∏è‚É£ Dataset Preview\n",
        "    dataset_preview = gr.Dataframe(\n",
        "        label=\"Preview of Generated Dataset\",\n",
        "        interactive=False\n",
        "    )\n",
        "\n",
        "    # 6Ô∏è‚É£ Download CSV Button (initially hidden)\n",
        "    download_button = gr.File(label=\"‚¨áÔ∏è Download as CSV\", visible=False)\n",
        "\n",
        "\n",
        "    # --- Connect the button to the backend ---\n",
        "    generate_btn.click(\n",
        "        fn=generate_dataset,\n",
        "        inputs=[user_input, model_dropdown, size_dropdown],\n",
        "        outputs=[generation_details, dataset_preview, download_button]\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "7Q_eQclRkYwL",
        "outputId": "49081d08-d5b0-43de-ff60-31a1b0a6690e"
      },
      "id": "7Q_eQclRkYwL",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://1714f89811564565af.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1714f89811564565af.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}